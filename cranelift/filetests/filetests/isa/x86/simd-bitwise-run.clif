test run
set enable_simd
target x86_64 skylake


; Shift left.

function %ishl_i64x2(i64x2, i32) -> i64x2 {
block0(v0:i64x2, v1:i32):
    v2 = ishl v0, v1
    return v2
}
; run: %ishl_i64x2([1 -1], 0) == [1 -1]
; run: %ishl_i64x2([1 -1], 1) == [2 -2]
; run: %ishl_i64x2([1 -1], 64) == [0 0]

function %ishl_i32x4(i32x4, i32) -> i32x4 {
block0(v0:i32x4, v1:i32):
    v2 = ishl v0, v1
    return v2
}
; run: %ishl_i32x4([1 2 4 8], 1) == [2 4 8 16]

function %ishl_i16x8(i16x8, i32) -> i16x8 {
block0(v0:i16x8, v1:i32):
    v2 = ishl v0, v1
    return v2
}
; run: %ishl_i16x8([1 2 4 8 16 32 64 128], 17) == [0 0 0 0 0 0 0 0]

function %ishl_i8x16(i8x16, i32) -> i8x16 {
block0(v0:i8x16, v1:i32):
    v2 = ishl v0, v1
    return v2
}
; run: %ishl_i8x16([0x80 0xc0 0 1 2 3 4 5 6 7 8 9 10 11 12 13], 1) == [0 0x80 0 2 4 6 8 10 12 14 16 18 20 22 24 26]
; run: %ishl_i8x16([0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15], 1) == [0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30]


; Shift right (logical).

function %ushr_i64x2(i64x2, i32) -> i64x2 {
block0(v0:i64x2, v1:i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i64x2([1 2], 1) == [0 1]

function %ushr_i32x4(i32x4, i32) -> i32x4 {
block0(v0:i32x4, v1:i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i32x4([1 2 4 8], 33) == [0 0 0 0]

function %ushr_i16x8(i16x8, i32) -> i16x8 {
block0(v0:i16x8, v1:i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i16x8([1 2 4 8 16 32 64 128], 17) == [0 0 0 0 0 0 0 0]

function %ushr_i8x16(i8x16, i32) -> i8x16 {
block0(v0:i8x16, v1:i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i8x16([0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15], 1) == [0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7]



; Shift right (arithmetic).

function %sshr_i64x2(i64x2, i32) -> i64x2 {
block0(v0:i64x2, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i64x2([1 -1], 0) == [1 -1]
; run: %sshr_i64x2([1 -1], 1) == [0 -1] ; note the -1 shift result
; run: %sshr_i64x2([2 -2], 1) == [1 -1]

function %sshr_i32x4(i32x4, i32) -> i32x4 {
block0(v0:i32x4, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i32x4([1 2 4 -8], 33) == [0 0 0 -1]
; run: %sshr_i32x4([1 2 4 -8], 1) == [0 1 2 -4]

function %sshr_i16x8(i16x8, i32) -> i16x8 {
block0(v0:i16x8, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i16x8([-1 2 4 8 -16 32 64 128], 1) == [-1 1 2 4 -8 16 32 64]

function %sshr_i8x16(i8x16, i32) -> i8x16 {
block0(v0:i8x16, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i8x16([0 0xff 2 0xfd 4 0xfb 6 0xf9 8 0xf7 10 0xf5 12 0xf3 14 0xf1], 1) == [0 0xff 1 0xfe 2 0xfd 3 0xfc 4 0xfb 5 0xfa 6 0xf9 7 0xf8]



; Bitselect.

function %bitselect_i8x16(i8x16, i8x16, i8x16) -> i8x16 {
block0(v0:i8x16, v1:i8x16, v2:i8x16):
    v3 = bitselect v0, v1, v2
    return v3
}
; run: %bitselect_i8x16([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 255],  [127 0 0 0 0 0 0 0 0 0 0 0 0 0 0 42], [42 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127]) ==  [42 0 0 0 0 0 0 0 0 0 0 0 0 0 0 42]



; Immediate shifts (assorted).

function %ishl_imm_i64x2(i64x2) -> i64x2 {
block0(v0:i64x2):
    v2 = ishl_imm v0, 1
    return v2
}
; run: %ishl_imm_i64x2([1 0]) == [2 0]

function %sshr_imm_i32x4(i32x4) -> i32x4 {
block0(v0:i32x4):
    v2 = sshr_imm v0, 1
    return v2
}
; run: %sshr_imm_i32x4([1 2 4 -8]) == [0 1 2 -4]

function %sshr_imm_i16x8(i16x8) -> i16x8 {
block0(v0:i16x8):
    v2 = sshr_imm v0, 1
    return v2
}
; run: %sshr_imm_i16x8([1 2 4 -8 0 0 0 0]) == [0 1 2 -4 0 0 0 0]
