package wasi:nn@0.2.0-rc-2024-06-25;

/// `wasi-nn` is a WASI API for performing machine learning (ML) inference. The API is not (yet)
/// capable of performing ML training. WebAssembly programs that want to use a host's ML
/// capabilities can access these capabilities through `wasi-nn`'s core abstractions: _graphs_ and
/// _tensors_. A user `load`s an ML model -- instantiated as a _graph_ -- to use in an ML _backend_.
/// Then, the user passes _tensor_ inputs to the _graph_, computes the inference, and retrieves the
/// _tensor_ outputs.
///
/// This example world shows how to use these primitives together.
world ml {
    import tensor;
    import graph;
    import inference;
    import prompt;
}

/// All inputs and outputs to an ML inference are represented as `tensor`s.
interface tensor {
    /// The dimensions of a tensor.
    ///
    /// The array length matches the tensor rank and each element in the array describes the size of
    /// each dimension
    type tensor-dimensions = list<u32>;

    /// The type of the elements in a tensor.
    enum tensor-type {
        FP16,
        FP32,
        FP64,
        BF16,
        U8,
        I32,
        I64
    }

    /// The tensor data.
    ///
    /// Initially conceived as a sparse representation, each empty cell would be filled with zeros
    /// and the array length must match the product of all of the dimensions and the number of bytes
    /// in the type (e.g., a 2x2 tensor with 4-byte f32 elements would have a data array of length
    /// 16). Naturally, this representation requires some knowledge of how to lay out data in
    /// memory--e.g., using row-major ordering--and could perhaps be improved.
    type tensor-data = list<u8>;

    resource tensor {
        constructor(dimensions: tensor-dimensions, ty: tensor-type, data: tensor-data);

        // Describe the size of the tensor (e.g., 2x2x2x2 -> [2, 2, 2, 2]). To represent a tensor
        // containing a single value, use `[1]` for the tensor dimensions.
        dimensions: func() -> tensor-dimensions;

        // Describe the type of element in the tensor (e.g., `f32`).
        ty: func() -> tensor-type;

        // Return the tensor data.
        data: func() -> tensor-data;
    }
}

/// A `graph` is a loaded instance of a specific ML model (e.g., MobileNet) for a specific ML
/// framework (e.g., TensorFlow):
interface graph {
    /// Load a `graph` by name.
    ///
    /// How the host expects the names to be passed and how it stores the graphs for retrieval via
    /// this function is **implementation-specific**. This allows hosts to choose name schemes that
    /// range from simple to complex (e.g., URLs?) and caching mechanisms of various kinds.
    load: func(name: string) -> result<graph, string>;

    /// An execution graph for performing inference (i.e., a model).
    resource graph {
        /// Retrieve the properties of the graph.
        ///
        /// These are metadata about the graph, unique to the graph and the
        /// ML backend providing it.
        list-properties: func() -> list<string>;

        /// Retrieve the value of a property.
        ///
        /// If the property does not exist, this function returns `none`.
        get-property: func(name: string) -> option<string>;

        /// Modify the value of a property.
        ///
        /// If the operation fails, this function returns a string from the ML
        /// backend describing the error.
        set-property: func(name: string, value: string) -> result<_, string>;
    }
}

/// An inference "session" is encapsulated by a `context`; use this to `compute`
/// an inference.
interface inference {
    use graph.{graph};
    use tensor.{tensor};

    /// Initialize an inference session with a graph.
    ///
    /// Note that not all graphs are inference-ready (see `prompt`); this
    /// function may fail in this case.
    init: func(graph: graph) -> result<context, string>;

    /// Identify a tensor by name; this is necessary to associate tensors to
    /// graph inputs and outputs.
    type named-tensor = tuple<string, tensor>;

    /// An inference "session."
    resource context {
        /// Compute an inference request with the given inputs.
        compute: func(inputs: list<named-tensor>) -> result<list<named-tensor>, string>;
    }
}

/// A prompt "session" is encapsulated by a `context`.
interface prompt {
    use graph.{graph};

    /// Initialize a prompt session with a graph.
    ///
    /// Note that not all graphs are prompt-ready (see `inference`); this
    /// function may fail in this case.
    init: func(graph: graph) -> result<context, string>;

    /// A prompt "session."
    resource context {
        /// Compute an inference request with the given inputs.
        compute: func(prompt: string) -> result<string, string>;
    }
}
